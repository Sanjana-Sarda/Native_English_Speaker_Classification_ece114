{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECE114_Speech_ML_project_example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoBdJNiyaMoC",
        "colab_type": "text"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6cfKHA7p3WP",
        "colab_type": "code",
        "outputId": "84832cc7-b8aa-454a-ed82-dfaa52acf8fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr1CQxtvaQTa",
        "colab_type": "text"
      },
      "source": [
        "Change Directory to Working Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX_Gzz72p-bH",
        "colab_type": "code",
        "outputId": "cf895049-876b-4b1b-b21b-3934ef999bcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "#print(os.getcwd())\n",
        "os.chdir('/content/gdrive/My Drive/114/Speech Project')\n",
        "!ls\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'All Sauce, No Juice'\t\t\t  feat_vec.mat\t        labels_vb.mat\n",
            " ECE114_Speech_ML_project_example.ipynb   feat_vec_rnn_vb.mat\n",
            " feat_vec2.mat\t\t\t\t  labels.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT9QjbtWaWa-",
        "colab_type": "text"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD-odBBaw613",
        "colab_type": "code",
        "outputId": "93d7acac-50cd-4aff-8894-be06445db66f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X=sio.loadmat('feat_vec2.mat')\n",
        "y=sio.loadmat('labels.mat')\n",
        "\n",
        "X_data=X['feat_vec']\n",
        "y_data=y['labels']\n",
        "\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test= train_test_split(X_data, y_data, test_size=0.15)\n",
        "\n",
        "X_train, X_val, y_train, y_val= train_test_split(X_train_val, y_train_val, test_size=0.15)\n",
        "\n",
        "\n",
        "\n",
        "print ('Training/Valid data shape: {}'.format(X_train_val.shape))\n",
        "print ('Test data shape: {}'.format(X_test.shape))\n",
        "print ('Training/Valid target shape: {}'.format(y_train_val.shape))\n",
        "print ('Test target shape: {}'.format(y_test.shape))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training/Valid data shape: (144, 5)\n",
            "Test data shape: (26, 5)\n",
            "Training/Valid target shape: (144, 1)\n",
            "Test target shape: (26, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UGP5EPvaYYJ",
        "colab_type": "text"
      },
      "source": [
        "One-hot encode labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ6w6PHUzh_o",
        "colab_type": "code",
        "outputId": "18b738e4-0c52-44f3-b3e6-a54de1d02b4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "\n",
        "\n",
        "y_train = onehot_encoder.fit_transform(y_train)\n",
        "y_val = onehot_encoder.fit_transform(y_val)\n",
        "y_test = onehot_encoder.fit_transform(y_test)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_train)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(122, 2)\n",
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRV3K8M5acwm",
        "colab_type": "text"
      },
      "source": [
        "Run Fully Connected (FeedForward) Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKIRxQRg1PMn",
        "colab_type": "code",
        "outputId": "a591836d-1cf8-4e28-fc88-86321864e026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(64, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(16, activation='sigmoid'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(2, activation='softmax'))     \n",
        "\n",
        "Adam=optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.9, amsgrad=False)\n",
        "Ad = optimizers.Adadelta()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Ad,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history=model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=100, epochs=100)\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 122 samples, validate on 22 samples\n",
            "Epoch 1/100\n",
            "122/122 [==============================] - 4s 33ms/step - loss: 0.7723 - acc: 0.5328 - val_loss: 0.7316 - val_acc: 0.5455\n",
            "Epoch 2/100\n",
            "122/122 [==============================] - 0s 204us/step - loss: 0.7550 - acc: 0.5246 - val_loss: 0.7268 - val_acc: 0.5455\n",
            "Epoch 3/100\n",
            "122/122 [==============================] - 0s 146us/step - loss: 0.7599 - acc: 0.5328 - val_loss: 0.7171 - val_acc: 0.5455\n",
            "Epoch 4/100\n",
            "122/122 [==============================] - 0s 148us/step - loss: 0.7239 - acc: 0.5328 - val_loss: 0.7059 - val_acc: 0.5455\n",
            "Epoch 5/100\n",
            "122/122 [==============================] - 0s 128us/step - loss: 0.7109 - acc: 0.5410 - val_loss: 0.7161 - val_acc: 0.5455\n",
            "Epoch 6/100\n",
            "122/122 [==============================] - 0s 154us/step - loss: 0.7236 - acc: 0.5164 - val_loss: 0.6895 - val_acc: 0.5455\n",
            "Epoch 7/100\n",
            "122/122 [==============================] - 0s 143us/step - loss: 0.6673 - acc: 0.5820 - val_loss: 0.6876 - val_acc: 0.5455\n",
            "Epoch 8/100\n",
            "122/122 [==============================] - 0s 187us/step - loss: 0.7011 - acc: 0.5574 - val_loss: 0.6849 - val_acc: 0.5455\n",
            "Epoch 9/100\n",
            "122/122 [==============================] - 0s 159us/step - loss: 0.6828 - acc: 0.5656 - val_loss: 0.6845 - val_acc: 0.5455\n",
            "Epoch 10/100\n",
            "122/122 [==============================] - 0s 159us/step - loss: 0.7043 - acc: 0.4918 - val_loss: 0.6866 - val_acc: 0.5455\n",
            "Epoch 11/100\n",
            "122/122 [==============================] - 0s 154us/step - loss: 0.7241 - acc: 0.4836 - val_loss: 0.6905 - val_acc: 0.5455\n",
            "Epoch 12/100\n",
            "122/122 [==============================] - 0s 151us/step - loss: 0.7112 - acc: 0.5082 - val_loss: 0.6863 - val_acc: 0.5455\n",
            "Epoch 13/100\n",
            "122/122 [==============================] - 0s 163us/step - loss: 0.7025 - acc: 0.5246 - val_loss: 0.6826 - val_acc: 0.5455\n",
            "Epoch 14/100\n",
            "122/122 [==============================] - 0s 185us/step - loss: 0.6808 - acc: 0.5328 - val_loss: 0.6873 - val_acc: 0.5455\n",
            "Epoch 15/100\n",
            "122/122 [==============================] - 0s 168us/step - loss: 0.7109 - acc: 0.4836 - val_loss: 0.6942 - val_acc: 0.5455\n",
            "Epoch 16/100\n",
            "122/122 [==============================] - 0s 182us/step - loss: 0.7276 - acc: 0.4426 - val_loss: 0.6948 - val_acc: 0.5455\n",
            "Epoch 17/100\n",
            "122/122 [==============================] - 0s 167us/step - loss: 0.7164 - acc: 0.4672 - val_loss: 0.6915 - val_acc: 0.5455\n",
            "Epoch 18/100\n",
            "122/122 [==============================] - 0s 185us/step - loss: 0.6870 - acc: 0.5984 - val_loss: 0.6918 - val_acc: 0.5455\n",
            "Epoch 19/100\n",
            "122/122 [==============================] - 0s 168us/step - loss: 0.7242 - acc: 0.4508 - val_loss: 0.6932 - val_acc: 0.5455\n",
            "Epoch 20/100\n",
            "122/122 [==============================] - 0s 144us/step - loss: 0.6990 - acc: 0.4918 - val_loss: 0.6908 - val_acc: 0.5455\n",
            "Epoch 21/100\n",
            "122/122 [==============================] - 0s 173us/step - loss: 0.7009 - acc: 0.5000 - val_loss: 0.6898 - val_acc: 0.5455\n",
            "Epoch 22/100\n",
            "122/122 [==============================] - 0s 157us/step - loss: 0.6944 - acc: 0.5492 - val_loss: 0.6895 - val_acc: 0.5455\n",
            "Epoch 23/100\n",
            "122/122 [==============================] - 0s 227us/step - loss: 0.6927 - acc: 0.4918 - val_loss: 0.6899 - val_acc: 0.5455\n",
            "Epoch 24/100\n",
            "122/122 [==============================] - 0s 141us/step - loss: 0.6971 - acc: 0.5246 - val_loss: 0.6898 - val_acc: 0.5455\n",
            "Epoch 25/100\n",
            "122/122 [==============================] - 0s 151us/step - loss: 0.7272 - acc: 0.4754 - val_loss: 0.6909 - val_acc: 0.5455\n",
            "Epoch 26/100\n",
            "122/122 [==============================] - 0s 188us/step - loss: 0.7049 - acc: 0.4918 - val_loss: 0.6914 - val_acc: 0.5455\n",
            "Epoch 27/100\n",
            "122/122 [==============================] - 0s 158us/step - loss: 0.6792 - acc: 0.5328 - val_loss: 0.6916 - val_acc: 0.5455\n",
            "Epoch 28/100\n",
            "122/122 [==============================] - 0s 161us/step - loss: 0.6927 - acc: 0.5492 - val_loss: 0.6890 - val_acc: 0.5455\n",
            "Epoch 29/100\n",
            "122/122 [==============================] - 0s 197us/step - loss: 0.6983 - acc: 0.5328 - val_loss: 0.6922 - val_acc: 0.5455\n",
            "Epoch 30/100\n",
            "122/122 [==============================] - 0s 159us/step - loss: 0.7194 - acc: 0.4426 - val_loss: 0.6929 - val_acc: 0.5455\n",
            "Epoch 31/100\n",
            "122/122 [==============================] - 0s 152us/step - loss: 0.6858 - acc: 0.5410 - val_loss: 0.6950 - val_acc: 0.5455\n",
            "Epoch 32/100\n",
            "122/122 [==============================] - 0s 161us/step - loss: 0.6978 - acc: 0.5246 - val_loss: 0.6898 - val_acc: 0.5455\n",
            "Epoch 33/100\n",
            "122/122 [==============================] - 0s 180us/step - loss: 0.6863 - acc: 0.5656 - val_loss: 0.6918 - val_acc: 0.5455\n",
            "Epoch 34/100\n",
            "122/122 [==============================] - 0s 157us/step - loss: 0.7048 - acc: 0.5328 - val_loss: 0.6845 - val_acc: 0.5455\n",
            "Epoch 35/100\n",
            "122/122 [==============================] - 0s 194us/step - loss: 0.7064 - acc: 0.5410 - val_loss: 0.6893 - val_acc: 0.5455\n",
            "Epoch 36/100\n",
            "122/122 [==============================] - 0s 184us/step - loss: 0.6994 - acc: 0.5656 - val_loss: 0.6914 - val_acc: 0.5455\n",
            "Epoch 37/100\n",
            "122/122 [==============================] - 0s 128us/step - loss: 0.7142 - acc: 0.5656 - val_loss: 0.6895 - val_acc: 0.5455\n",
            "Epoch 38/100\n",
            "122/122 [==============================] - 0s 152us/step - loss: 0.7053 - acc: 0.5082 - val_loss: 0.6924 - val_acc: 0.5455\n",
            "Epoch 39/100\n",
            "122/122 [==============================] - 0s 158us/step - loss: 0.6883 - acc: 0.5492 - val_loss: 0.6934 - val_acc: 0.4545\n",
            "Epoch 40/100\n",
            "122/122 [==============================] - 0s 163us/step - loss: 0.6909 - acc: 0.5410 - val_loss: 0.6949 - val_acc: 0.4545\n",
            "Epoch 41/100\n",
            "122/122 [==============================] - 0s 160us/step - loss: 0.6860 - acc: 0.5656 - val_loss: 0.6924 - val_acc: 0.5455\n",
            "Epoch 42/100\n",
            "122/122 [==============================] - 0s 151us/step - loss: 0.6973 - acc: 0.5246 - val_loss: 0.6900 - val_acc: 0.5455\n",
            "Epoch 43/100\n",
            "122/122 [==============================] - 0s 178us/step - loss: 0.7007 - acc: 0.4590 - val_loss: 0.6919 - val_acc: 0.5455\n",
            "Epoch 44/100\n",
            "122/122 [==============================] - 0s 161us/step - loss: 0.7012 - acc: 0.4426 - val_loss: 0.6909 - val_acc: 0.5455\n",
            "Epoch 45/100\n",
            "122/122 [==============================] - 0s 138us/step - loss: 0.6746 - acc: 0.5492 - val_loss: 0.6894 - val_acc: 0.5455\n",
            "Epoch 46/100\n",
            "122/122 [==============================] - 0s 163us/step - loss: 0.6891 - acc: 0.5082 - val_loss: 0.6910 - val_acc: 0.5455\n",
            "Epoch 47/100\n",
            "122/122 [==============================] - 0s 155us/step - loss: 0.7048 - acc: 0.4672 - val_loss: 0.6922 - val_acc: 0.5455\n",
            "Epoch 48/100\n",
            "122/122 [==============================] - 0s 163us/step - loss: 0.7168 - acc: 0.4918 - val_loss: 0.6910 - val_acc: 0.5455\n",
            "Epoch 49/100\n",
            "122/122 [==============================] - 0s 129us/step - loss: 0.6940 - acc: 0.5328 - val_loss: 0.6922 - val_acc: 0.5455\n",
            "Epoch 50/100\n",
            "122/122 [==============================] - 0s 145us/step - loss: 0.7129 - acc: 0.5082 - val_loss: 0.6954 - val_acc: 0.4545\n",
            "Epoch 51/100\n",
            "122/122 [==============================] - 0s 162us/step - loss: 0.6878 - acc: 0.4836 - val_loss: 0.6931 - val_acc: 0.5455\n",
            "Epoch 52/100\n",
            "122/122 [==============================] - 0s 169us/step - loss: 0.6981 - acc: 0.4918 - val_loss: 0.6901 - val_acc: 0.5455\n",
            "Epoch 53/100\n",
            "122/122 [==============================] - 0s 137us/step - loss: 0.6703 - acc: 0.5492 - val_loss: 0.6892 - val_acc: 0.5455\n",
            "Epoch 54/100\n",
            "122/122 [==============================] - 0s 182us/step - loss: 0.7199 - acc: 0.4836 - val_loss: 0.6901 - val_acc: 0.5455\n",
            "Epoch 55/100\n",
            "122/122 [==============================] - 0s 173us/step - loss: 0.7026 - acc: 0.5246 - val_loss: 0.6894 - val_acc: 0.5455\n",
            "Epoch 56/100\n",
            "122/122 [==============================] - 0s 211us/step - loss: 0.6944 - acc: 0.5410 - val_loss: 0.6900 - val_acc: 0.5455\n",
            "Epoch 57/100\n",
            "122/122 [==============================] - 0s 197us/step - loss: 0.6966 - acc: 0.5000 - val_loss: 0.6912 - val_acc: 0.5455\n",
            "Epoch 58/100\n",
            "122/122 [==============================] - 0s 151us/step - loss: 0.6987 - acc: 0.5164 - val_loss: 0.6909 - val_acc: 0.5455\n",
            "Epoch 59/100\n",
            "122/122 [==============================] - 0s 162us/step - loss: 0.6939 - acc: 0.5492 - val_loss: 0.6898 - val_acc: 0.5455\n",
            "Epoch 60/100\n",
            "122/122 [==============================] - 0s 157us/step - loss: 0.6818 - acc: 0.5738 - val_loss: 0.6894 - val_acc: 0.5455\n",
            "Epoch 61/100\n",
            "122/122 [==============================] - 0s 149us/step - loss: 0.6803 - acc: 0.5410 - val_loss: 0.6898 - val_acc: 0.5455\n",
            "Epoch 62/100\n",
            "122/122 [==============================] - 0s 143us/step - loss: 0.6924 - acc: 0.5164 - val_loss: 0.6892 - val_acc: 0.5455\n",
            "Epoch 63/100\n",
            "122/122 [==============================] - 0s 161us/step - loss: 0.6966 - acc: 0.5164 - val_loss: 0.6894 - val_acc: 0.5455\n",
            "Epoch 64/100\n",
            "122/122 [==============================] - 0s 163us/step - loss: 0.6921 - acc: 0.5410 - val_loss: 0.6892 - val_acc: 0.5455\n",
            "Epoch 65/100\n",
            "122/122 [==============================] - 0s 172us/step - loss: 0.7029 - acc: 0.5082 - val_loss: 0.6902 - val_acc: 0.5455\n",
            "Epoch 66/100\n",
            "122/122 [==============================] - 0s 194us/step - loss: 0.7221 - acc: 0.4262 - val_loss: 0.6894 - val_acc: 0.5455\n",
            "Epoch 67/100\n",
            "122/122 [==============================] - 0s 158us/step - loss: 0.7180 - acc: 0.5082 - val_loss: 0.6899 - val_acc: 0.5455\n",
            "Epoch 68/100\n",
            "122/122 [==============================] - 0s 164us/step - loss: 0.7022 - acc: 0.5000 - val_loss: 0.6890 - val_acc: 0.5455\n",
            "Epoch 69/100\n",
            "122/122 [==============================] - 0s 175us/step - loss: 0.6883 - acc: 0.5738 - val_loss: 0.6890 - val_acc: 0.5455\n",
            "Epoch 70/100\n",
            "122/122 [==============================] - 0s 276us/step - loss: 0.6951 - acc: 0.5656 - val_loss: 0.6900 - val_acc: 0.5455\n",
            "Epoch 71/100\n",
            "122/122 [==============================] - 0s 168us/step - loss: 0.6815 - acc: 0.5984 - val_loss: 0.6825 - val_acc: 0.5455\n",
            "Epoch 72/100\n",
            "122/122 [==============================] - 0s 190us/step - loss: 0.7135 - acc: 0.4918 - val_loss: 0.6827 - val_acc: 0.5455\n",
            "Epoch 73/100\n",
            "122/122 [==============================] - 0s 173us/step - loss: 0.6919 - acc: 0.5574 - val_loss: 0.6867 - val_acc: 0.5455\n",
            "Epoch 74/100\n",
            "122/122 [==============================] - 0s 173us/step - loss: 0.6839 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5455\n",
            "Epoch 75/100\n",
            "122/122 [==============================] - 0s 179us/step - loss: 0.6840 - acc: 0.5738 - val_loss: 0.6857 - val_acc: 0.5455\n",
            "Epoch 76/100\n",
            "122/122 [==============================] - 0s 179us/step - loss: 0.6810 - acc: 0.5328 - val_loss: 0.6802 - val_acc: 0.5455\n",
            "Epoch 77/100\n",
            "122/122 [==============================] - 0s 161us/step - loss: 0.7058 - acc: 0.5164 - val_loss: 0.6793 - val_acc: 0.5455\n",
            "Epoch 78/100\n",
            "122/122 [==============================] - 0s 161us/step - loss: 0.7096 - acc: 0.5574 - val_loss: 0.6775 - val_acc: 0.5455\n",
            "Epoch 79/100\n",
            "122/122 [==============================] - 0s 139us/step - loss: 0.6762 - acc: 0.5738 - val_loss: 0.6660 - val_acc: 0.5455\n",
            "Epoch 80/100\n",
            "122/122 [==============================] - 0s 198us/step - loss: 0.6746 - acc: 0.5820 - val_loss: 0.6797 - val_acc: 0.5455\n",
            "Epoch 81/100\n",
            "122/122 [==============================] - 0s 180us/step - loss: 0.6695 - acc: 0.5738 - val_loss: 0.6741 - val_acc: 0.5455\n",
            "Epoch 82/100\n",
            "122/122 [==============================] - 0s 166us/step - loss: 0.6886 - acc: 0.5082 - val_loss: 0.6797 - val_acc: 0.5455\n",
            "Epoch 83/100\n",
            "122/122 [==============================] - 0s 171us/step - loss: 0.6832 - acc: 0.6066 - val_loss: 0.6835 - val_acc: 0.6818\n",
            "Epoch 84/100\n",
            "122/122 [==============================] - 0s 187us/step - loss: 0.6959 - acc: 0.5492 - val_loss: 0.6850 - val_acc: 0.6364\n",
            "Epoch 85/100\n",
            "122/122 [==============================] - 0s 182us/step - loss: 0.7006 - acc: 0.5246 - val_loss: 0.6755 - val_acc: 0.5455\n",
            "Epoch 86/100\n",
            "122/122 [==============================] - 0s 166us/step - loss: 0.6657 - acc: 0.5574 - val_loss: 0.6731 - val_acc: 0.7727\n",
            "Epoch 87/100\n",
            "122/122 [==============================] - 0s 174us/step - loss: 0.7084 - acc: 0.5164 - val_loss: 0.6900 - val_acc: 0.5909\n",
            "Epoch 88/100\n",
            "122/122 [==============================] - 0s 157us/step - loss: 0.7100 - acc: 0.4754 - val_loss: 0.6798 - val_acc: 0.6818\n",
            "Epoch 89/100\n",
            "122/122 [==============================] - 0s 177us/step - loss: 0.6875 - acc: 0.5082 - val_loss: 0.6843 - val_acc: 0.5455\n",
            "Epoch 90/100\n",
            "122/122 [==============================] - 0s 166us/step - loss: 0.6860 - acc: 0.5328 - val_loss: 0.6791 - val_acc: 0.5455\n",
            "Epoch 91/100\n",
            "122/122 [==============================] - 0s 177us/step - loss: 0.6708 - acc: 0.5656 - val_loss: 0.6906 - val_acc: 0.5909\n",
            "Epoch 92/100\n",
            "122/122 [==============================] - 0s 170us/step - loss: 0.6789 - acc: 0.5574 - val_loss: 0.6907 - val_acc: 0.5000\n",
            "Epoch 93/100\n",
            "122/122 [==============================] - 0s 158us/step - loss: 0.6677 - acc: 0.5738 - val_loss: 0.6914 - val_acc: 0.5455\n",
            "Epoch 94/100\n",
            "122/122 [==============================] - 0s 209us/step - loss: 0.7170 - acc: 0.3934 - val_loss: 0.6797 - val_acc: 0.5455\n",
            "Epoch 95/100\n",
            "122/122 [==============================] - 0s 187us/step - loss: 0.6817 - acc: 0.5902 - val_loss: 0.6806 - val_acc: 0.5455\n",
            "Epoch 96/100\n",
            "122/122 [==============================] - 0s 176us/step - loss: 0.6890 - acc: 0.5738 - val_loss: 0.6841 - val_acc: 0.5455\n",
            "Epoch 97/100\n",
            "122/122 [==============================] - 0s 142us/step - loss: 0.6618 - acc: 0.6475 - val_loss: 0.6799 - val_acc: 0.5455\n",
            "Epoch 98/100\n",
            "122/122 [==============================] - 0s 164us/step - loss: 0.6968 - acc: 0.5000 - val_loss: 0.6785 - val_acc: 0.5455\n",
            "Epoch 99/100\n",
            "122/122 [==============================] - 0s 160us/step - loss: 0.6937 - acc: 0.5328 - val_loss: 0.6795 - val_acc: 0.5455\n",
            "Epoch 100/100\n",
            "122/122 [==============================] - 0s 175us/step - loss: 0.6923 - acc: 0.5656 - val_loss: 0.6812 - val_acc: 0.5455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EebZ47qMakj3",
        "colab_type": "text"
      },
      "source": [
        "Get validation and test scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuWhLcrO5WFB",
        "colab_type": "code",
        "outputId": "7a16c2c5-c207-4958-ebe3-9c8b7805f337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "score_val = model.evaluate(X_val, y_val, batch_size=20)\n",
        "score_test = model.evaluate(X_test, y_test, batch_size=20)\n",
        "\n",
        "print('Validation [loss, accuracy] is ', score_val)\n",
        "print('Test [loss, accuracy] is ', score_test)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22/22 [==============================] - 0s 393us/step\n",
            "26/26 [==============================] - 0s 377us/step\n",
            "Validation [loss, accuracy] is  [0.6812004988843744, 0.5454545562917535]\n",
            "Test [loss, accuracy] is  [0.7015493512153625, 0.5000000114624317]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89iHOYkaapTo",
        "colab_type": "text"
      },
      "source": [
        "Get data formatted for RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b397hiNG0pY",
        "colab_type": "code",
        "outputId": "8ae85ce9-92bc-45db-ee66-7546f1b6a3a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "X=sio.loadmat('feat_vec_rnn_vb.mat')\n",
        "y=sio.loadmat('labels_vb.mat')\n",
        "\n",
        "\n",
        "\n",
        "X_data=X['feat_vec_rnn']\n",
        "X_data=np.transpose(X_data, (2, 0, 1))\n",
        "\n",
        "y_data=y['labels']\n",
        "\n",
        "print(X_data.shape)\n",
        "print(y_data.shape)\n",
        "\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test= train_test_split(X_data, y_data, test_size=0.15)\n",
        "\n",
        "X_train, X_val, y_train, y_val= train_test_split(X_train_val, y_train_val, test_size=0.15)\n",
        "\n",
        "\n",
        "\n",
        "print ('Training/Valid data shape: {}'.format(X_train_val.shape))\n",
        "print ('Test data shape: {}'.format(X_test.shape))\n",
        "print ('Training/Valid target shape: {}'.format(y_train_val.shape))\n",
        "print ('Test target shape: {}'.format(y_test.shape))\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "y_train = onehot_encoder.fit_transform(y_train)\n",
        "y_val = onehot_encoder.fit_transform(y_val)\n",
        "y_test = onehot_encoder.fit_transform(y_test)\n",
        "\n",
        "print(y_train.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(170, 1363, 12)\n",
            "(170, 1)\n",
            "Training/Valid data shape: (144, 1363, 12)\n",
            "Test data shape: (26, 1363, 12)\n",
            "Training/Valid target shape: (144, 1)\n",
            "Test target shape: (26, 1)\n",
            "(122, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXHr6znGasSG",
        "colab_type": "text"
      },
      "source": [
        "Run LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3Nd9Djw7rZR",
        "colab_type": "code",
        "outputId": "fcf52906-ea48-4298-cb11-ee0fe7cb1cce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import CuDNNLSTM\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "\n",
        "n_samples = X_train.shape[0]  # number of data points\n",
        "n_features = X_train.shape[1]  # dimension of feature vector for each sample\n",
        "time_steps = X_train.shape[2] # how many samples across time were taken\n",
        "\n",
        "#model = Sequential()\n",
        "#ss changed dropout to 0.1, stride to 1, pool to 3\n",
        "#model.add(LSTM(64, dropout=0.1, recurrent_dropout=0.1, return_sequences=True, kernel_initializer='random_normal', input_shape=(n_features, time_steps)))\n",
        "\n",
        "#model.add(MaxPooling1D(pool_size=(3), strides=1, padding='valid', data_format=None))\n",
        "\n",
        "#model.add(LSTM(64, return_sequences=True))\n",
        "\n",
        "#model.add(LSTM(64, return_sequences=False))\n",
        "\n",
        "#model.add(Dense(2, activation='softmax', kernel_regularizer=regularizers.l1_l2(l1=0,l2=0.5)))\n",
        "\n",
        "#model.add(LSTM(64, return_sequences=True, stateful=False, input_shape=(n_features, time_steps)))\n",
        "#model.add(LSTM(64, return_sequences=True, stateful=False))\n",
        "#model.add(LSTM(64, stateful=False))\n",
        "#model.add(Dropout(.25))\n",
        "#model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "#Adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.99, amsgrad=False)\n",
        "\n",
        "#model.compile(loss='categorical_crossentropy',\n",
        "     #         optimizer=Adam,\n",
        "     #         metrics=['accuracy'])\n",
        "\n",
        "#history=model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=100, epochs=15)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(64, dropout=0.1, recurrent_dropout=0.0, return_sequences=True, input_shape=(n_features, time_steps)))\n",
        "\n",
        "model.add(MaxPooling1D(pool_size=(5), strides=1, padding='valid', data_format=None))\n",
        "\n",
        "#model.add(LSTM(64, return_sequences=True))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(LSTM(30, return_sequences=False))\n",
        "\n",
        "model.add(Dense(2, activation='softmax', kernel_regularizer=regularizers.l1_l2(l1=0,l2=0.5)))\n",
        "\n",
        "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "Adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.99, amsgrad=False)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history=model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=100, epochs=10)\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Train on 122 samples, validate on 22 samples\n",
            "Epoch 1/10\n",
            "122/122 [==============================] - 23s 191ms/step - loss: 2.7128 - acc: 0.5328 - val_loss: 2.6975 - val_acc: 0.5909\n",
            "Epoch 2/10\n",
            "122/122 [==============================] - 19s 152ms/step - loss: 2.6725 - acc: 0.6148 - val_loss: 2.6868 - val_acc: 0.6364\n",
            "Epoch 3/10\n",
            "122/122 [==============================] - 19s 152ms/step - loss: 2.6637 - acc: 0.5984 - val_loss: 2.6812 - val_acc: 0.6364\n",
            "Epoch 4/10\n",
            "122/122 [==============================] - 19s 153ms/step - loss: 2.6563 - acc: 0.5902 - val_loss: 2.6771 - val_acc: 0.6364\n",
            "Epoch 5/10\n",
            "122/122 [==============================] - 19s 153ms/step - loss: 2.6555 - acc: 0.5738 - val_loss: 2.6741 - val_acc: 0.6364\n",
            "Epoch 6/10\n",
            "122/122 [==============================] - 19s 153ms/step - loss: 2.6492 - acc: 0.5738 - val_loss: 2.6715 - val_acc: 0.6364\n",
            "Epoch 7/10\n",
            "122/122 [==============================] - 19s 153ms/step - loss: 2.6421 - acc: 0.5820 - val_loss: 2.6696 - val_acc: 0.6364\n",
            "Epoch 8/10\n",
            "122/122 [==============================] - 19s 153ms/step - loss: 2.6438 - acc: 0.5574 - val_loss: 2.6678 - val_acc: 0.6364\n",
            "Epoch 9/10\n",
            "122/122 [==============================] - 19s 152ms/step - loss: 2.6425 - acc: 0.5738 - val_loss: 2.6664 - val_acc: 0.6364\n",
            "Epoch 10/10\n",
            "122/122 [==============================] - 19s 152ms/step - loss: 2.6344 - acc: 0.5902 - val_loss: 2.6652 - val_acc: 0.6364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ajihnp6auW2",
        "colab_type": "text"
      },
      "source": [
        "Get val and test scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIU-beqBPHhb",
        "colab_type": "code",
        "outputId": "04bb73df-adfc-488d-cf9f-11ad25724feb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "score_val = model.evaluate(X_val, y_val, batch_size=20)\n",
        "score_test = model.evaluate(X_test, y_test, batch_size=20)\n",
        "\n",
        "print('Validation [loss, accuracy] is ', score_val)\n",
        "print('Test [loss, accuracy] is ', score_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22/22 [==============================] - 6s 275ms/step\n",
            "26/26 [==============================] - 6s 232ms/step\n",
            "Validation [loss, accuracy] is  [2.6651801629499956, 0.6363636580380526]\n",
            "Test [loss, accuracy] is  [2.5923770391024075, 0.8461538599087641]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}